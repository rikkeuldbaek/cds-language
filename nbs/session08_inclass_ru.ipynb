{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 8 - Language modelling with RNNs (Text Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we make produce **headlines** from **NY times headlines**????? :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 15:17:32.190477: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# data processing tools \n",
    "import string, os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# keras module for building LSTM \n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import tensorflow.keras.utils as ku \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# surpress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions\n",
    "Predefined functions that Ross made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard coding practice\n",
    "def clean_text(txt): #for each text\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower() #return vocab if it is not a part string.punctuation (str.punc== alle dumme tegn), essentially slet alle dumme tegn og lowercase\n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore') #make encoding  utf8 :))\n",
    "    return txt  # NOTE this will return nonsense headings at times, since some headlines are connected to questions\n",
    "\n",
    "def get_sequence_of_tokens(tokenizer, corpus):\n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences\n",
    "\n",
    "def generate_padded_sequences(input_sequences):\n",
    "    # get the length of the longest sequence\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    # make every sequence the length of the longest on\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, \n",
    "                                            maxlen=max_sequence_len, \n",
    "                                            padding='pre'))\n",
    "\n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, \n",
    "                            num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential() #sequential model, den tager ord for ord \n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words,  # creating embedding about each token, learned while training  \n",
    "                        10,  #small embedding, every word is represented by a 10 dimensional vector (hvilke ord ligger tættest på et givent ord i modellen)\n",
    "                        input_length=input_len))\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(100)) #long short term model\n",
    "    model.add(Dropout(0.1)) #during learning from the data and every iteration, remove 10% of the weights (90% of weights remains)  ### this is a finetuning parameter!!!!!!\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, #Dense layer =  output layer\n",
    "                    activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                    optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0] #get vocav\n",
    "        token_list = pad_sequences([token_list], \n",
    "                                    maxlen=max_sequence_len-1, \n",
    "                                    padding='pre') #pad them = overcome fixed dimensionality\n",
    "        predicted = np.argmax(model.predict(token_list),\n",
    "                                            axis=1) #\n",
    "        \n",
    "        output_word = \"\" #appending stuff and printing it to look nice\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"..\", \"..\", \"..\", \"431868\", \"news_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approveDate</th>\n",
       "      <th>commentBody</th>\n",
       "      <th>commentID</th>\n",
       "      <th>commentSequence</th>\n",
       "      <th>commentTitle</th>\n",
       "      <th>commentType</th>\n",
       "      <th>createDate</th>\n",
       "      <th>depth</th>\n",
       "      <th>editorsSelection</th>\n",
       "      <th>parentID</th>\n",
       "      <th>...</th>\n",
       "      <th>userLocation</th>\n",
       "      <th>userTitle</th>\n",
       "      <th>userURL</th>\n",
       "      <th>inReplyTo</th>\n",
       "      <th>articleID</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>newDesk</th>\n",
       "      <th>articleWordCount</th>\n",
       "      <th>printPage</th>\n",
       "      <th>typeOfMaterial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1491245186</td>\n",
       "      <td>This project makes me happy to be a 30+ year T...</td>\n",
       "      <td>22022598.0</td>\n",
       "      <td>22022598</td>\n",
       "      <td>&lt;br/&gt;</td>\n",
       "      <td>comment</td>\n",
       "      <td>1.491237e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Riverside, CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>58def1347c459f24986d7c80</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Insider</td>\n",
       "      <td>716.0</td>\n",
       "      <td>2</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1491188619</td>\n",
       "      <td>Stunning photos and reportage. Infuriating tha...</td>\n",
       "      <td>22017350.0</td>\n",
       "      <td>22017350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>comment</td>\n",
       "      <td>1.491180e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;br/&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>58def1347c459f24986d7c80</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Insider</td>\n",
       "      <td>716.0</td>\n",
       "      <td>2</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1491188617</td>\n",
       "      <td>Brilliant work from conception to execution. I...</td>\n",
       "      <td>22017334.0</td>\n",
       "      <td>22017334</td>\n",
       "      <td>&lt;br/&gt;</td>\n",
       "      <td>comment</td>\n",
       "      <td>1.491179e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Raleigh NC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>58def1347c459f24986d7c80</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Insider</td>\n",
       "      <td>716.0</td>\n",
       "      <td>2</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1491167820</td>\n",
       "      <td>NYT reporters should provide a contributor's l...</td>\n",
       "      <td>22015913.0</td>\n",
       "      <td>22015913</td>\n",
       "      <td>&lt;br/&gt;</td>\n",
       "      <td>comment</td>\n",
       "      <td>1.491150e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Missouri, USA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>58def1347c459f24986d7c80</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Insider</td>\n",
       "      <td>716.0</td>\n",
       "      <td>2</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1491167815</td>\n",
       "      <td>Could only have been done in print. Stunning.</td>\n",
       "      <td>22015466.0</td>\n",
       "      <td>22015466</td>\n",
       "      <td>&lt;br/&gt;</td>\n",
       "      <td>comment</td>\n",
       "      <td>1.491147e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Tucson, Arizona</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>58def1347c459f24986d7c80</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Insider</td>\n",
       "      <td>716.0</td>\n",
       "      <td>2</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243827</th>\n",
       "      <td>1493061963</td>\n",
       "      <td>Sorry, but pudding has nothing to do with it; ...</td>\n",
       "      <td>22257227.0</td>\n",
       "      <td>22257227</td>\n",
       "      <td>&lt;br/&gt;</td>\n",
       "      <td>userReply</td>\n",
       "      <td>1.493059e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>22255279.0</td>\n",
       "      <td>...</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22255279</td>\n",
       "      <td>58fd5c3d7c459f24986dbac5</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Culture</td>\n",
       "      <td>981.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243828</th>\n",
       "      <td>1493060948</td>\n",
       "      <td>While it would be quite punny to spell it \"des...</td>\n",
       "      <td>22257466.0</td>\n",
       "      <td>22257466</td>\n",
       "      <td>&lt;br/&gt;</td>\n",
       "      <td>userReply</td>\n",
       "      <td>1.493061e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>22255279.0</td>\n",
       "      <td>...</td>\n",
       "      <td>New York City</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22255279</td>\n",
       "      <td>58fd5c3d7c459f24986dbac5</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Culture</td>\n",
       "      <td>981.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243829</th>\n",
       "      <td>1493087619</td>\n",
       "      <td>See above comments. \"deserts\" is the proper.</td>\n",
       "      <td>22259265.0</td>\n",
       "      <td>22259265</td>\n",
       "      <td>&lt;br/&gt;</td>\n",
       "      <td>userReply</td>\n",
       "      <td>1.493076e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>22253014.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Boston</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22253014</td>\n",
       "      <td>58fd5c3d7c459f24986dbac5</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Culture</td>\n",
       "      <td>981.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243830</th>\n",
       "      <td>1493042801</td>\n",
       "      <td>John Rubinstein had two brief scenes with Joan...</td>\n",
       "      <td>22250099.0</td>\n",
       "      <td>22250099</td>\n",
       "      <td>&lt;br/&gt;</td>\n",
       "      <td>userReply</td>\n",
       "      <td>1.493019e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>22249901.0</td>\n",
       "      <td>...</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22249901</td>\n",
       "      <td>58fd5c3d7c459f24986dbac5</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Culture</td>\n",
       "      <td>981.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243831</th>\n",
       "      <td>1493042800</td>\n",
       "      <td>Ye gods and little fishes, Stu, you're right ....</td>\n",
       "      <td>22252040.0</td>\n",
       "      <td>22252040</td>\n",
       "      <td>&lt;br/&gt;</td>\n",
       "      <td>userReply</td>\n",
       "      <td>1.493039e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>22249901.0</td>\n",
       "      <td>...</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22249901</td>\n",
       "      <td>58fd5c3d7c459f24986dbac5</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Culture</td>\n",
       "      <td>981.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Review</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243832 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        approveDate                                        commentBody  \\\n",
       "0        1491245186  This project makes me happy to be a 30+ year T...   \n",
       "1        1491188619  Stunning photos and reportage. Infuriating tha...   \n",
       "2        1491188617  Brilliant work from conception to execution. I...   \n",
       "3        1491167820  NYT reporters should provide a contributor's l...   \n",
       "4        1491167815     Could only have been done in print. Stunning.    \n",
       "...             ...                                                ...   \n",
       "243827   1493061963  Sorry, but pudding has nothing to do with it; ...   \n",
       "243828   1493060948  While it would be quite punny to spell it \"des...   \n",
       "243829   1493087619       See above comments. \"deserts\" is the proper.   \n",
       "243830   1493042801  John Rubinstein had two brief scenes with Joan...   \n",
       "243831   1493042800  Ye gods and little fishes, Stu, you're right ....   \n",
       "\n",
       "         commentID  commentSequence commentTitle commentType    createDate  \\\n",
       "0       22022598.0         22022598        <br/>     comment  1.491237e+09   \n",
       "1       22017350.0         22017350          NaN     comment  1.491180e+09   \n",
       "2       22017334.0         22017334        <br/>     comment  1.491179e+09   \n",
       "3       22015913.0         22015913        <br/>     comment  1.491150e+09   \n",
       "4       22015466.0         22015466        <br/>     comment  1.491147e+09   \n",
       "...            ...              ...          ...         ...           ...   \n",
       "243827  22257227.0         22257227        <br/>   userReply  1.493059e+09   \n",
       "243828  22257466.0         22257466        <br/>   userReply  1.493061e+09   \n",
       "243829  22259265.0         22259265        <br/>   userReply  1.493076e+09   \n",
       "243830  22250099.0         22250099        <br/>   userReply  1.493019e+09   \n",
       "243831  22252040.0         22252040        <br/>   userReply  1.493039e+09   \n",
       "\n",
       "        depth  editorsSelection    parentID  ...     userLocation userTitle  \\\n",
       "0           1             False         0.0  ...    Riverside, CA       NaN   \n",
       "1           1             False         0.0  ...            <br/>       NaN   \n",
       "2           1             False         0.0  ...       Raleigh NC       NaN   \n",
       "3           1             False         0.0  ...    Missouri, USA       NaN   \n",
       "4           1             False         0.0  ...  Tucson, Arizona       NaN   \n",
       "...       ...               ...         ...  ...              ...       ...   \n",
       "243827      2             False  22255279.0  ...     New York, NY       NaN   \n",
       "243828      2             False  22255279.0  ...    New York City       NaN   \n",
       "243829      2             False  22253014.0  ...           Boston       NaN   \n",
       "243830      2             False  22249901.0  ...     New York, NY       NaN   \n",
       "243831      2             False  22249901.0  ...       New Jersey       NaN   \n",
       "\n",
       "       userURL  inReplyTo                 articleID  sectionName  newDesk  \\\n",
       "0          NaN          0  58def1347c459f24986d7c80      Unknown  Insider   \n",
       "1          NaN          0  58def1347c459f24986d7c80      Unknown  Insider   \n",
       "2          NaN          0  58def1347c459f24986d7c80      Unknown  Insider   \n",
       "3          NaN          0  58def1347c459f24986d7c80      Unknown  Insider   \n",
       "4          NaN          0  58def1347c459f24986d7c80      Unknown  Insider   \n",
       "...        ...        ...                       ...          ...      ...   \n",
       "243827     NaN   22255279  58fd5c3d7c459f24986dbac5      Unknown  Culture   \n",
       "243828     NaN   22255279  58fd5c3d7c459f24986dbac5      Unknown  Culture   \n",
       "243829     NaN   22253014  58fd5c3d7c459f24986dbac5      Unknown  Culture   \n",
       "243830     NaN   22249901  58fd5c3d7c459f24986dbac5      Unknown  Culture   \n",
       "243831     NaN   22249901  58fd5c3d7c459f24986dbac5      Unknown  Culture   \n",
       "\n",
       "        articleWordCount printPage  typeOfMaterial  \n",
       "0                  716.0         2            News  \n",
       "1                  716.0         2            News  \n",
       "2                  716.0         2            News  \n",
       "3                  716.0         2            News  \n",
       "4                  716.0         2            News  \n",
       "...                  ...       ...             ...  \n",
       "243827             981.0         2          Review  \n",
       "243828             981.0         2          Review  \n",
       "243829             981.0         2          Review  \n",
       "243830             981.0         2          Review  \n",
       "243831             981.0         2          Review  \n",
       "\n",
       "[243832 rows x 34 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(data_dir + \"/\"+ \"ArticlesApril2017.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're then going to load the data one at a time and append *only* the headlines to our list of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headlines = []\n",
    "for filename in os.listdir(data_dir):\n",
    "    if 'Articles' in filename:\n",
    "        article_df = pd.read_csv(data_dir + filename)\n",
    "        all_headlines.extend(list(article_df[\"headline\"].values))#keep headline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then clean up a little bit and see how many data points we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headlines = [h for h in all_headlines if h != \"Unknown\"] #list comprehension syntax (if the headline is unknown, remove it)\n",
    "len(all_headlines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call out ```clean_text()``` function and then inspect the first 10 texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [clean_text(x) for x in all_headlines]\n",
    "corpus[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "We're then going to tokenize our data, using the ```Tokenizer()``` class from ```TensorFlow```, about which you can read more [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer).\n",
    "\n",
    "We then use the ```get_sequence_of_tokens()``` function we defined above, which turns every text into a sequence of tokens based on the vocabulary from the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: tensorflow also has a cleaning function--> it also can clean the data (u can use this instead of the predefined function)\n",
    "\n",
    "tokenizer = Tokenizer() \n",
    "## tokenization\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index #ordered after frequency, e.g. \"the\" is the most frequent word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_sequences = get_sequence_of_tokens(tokenizer, corpus)\n",
    "inp_sequences[:10] #every headline is represented by the index of the word (sounds messy)\n",
    "#ok\n",
    "# 46, 1601,1,  are three words and their index e.g\n",
    "#it then shows the \"eskalering\" af ord for ord indtil sætningen er constructed\n",
    "\n",
    "\n",
    "(len(inp_sequences[1:10])) #maaaany input sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to *pad* our input sequences to make them all the same length.\n",
    "- Fordi vi er løbet ind i problemet med FIXED DIMENSIONALITY == how to fix, add a 0 :))\n",
    "\n",
    "e.g.\n",
    "sequence1 = my cat\n",
    "sequence2 = my big cat\n",
    "these are uneven in length, how to fix? add a 0\n",
    "\n",
    "sequence1 = my cat 0\n",
    "sequence2 = my big cat\n",
    "\n",
    ".. always make sure to add so many 0's, to the longest sequence is covered\n",
    "e.g. longest seq\n",
    "sequence3= my big fat cat\n",
    "then = my cat 0 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences) \n",
    "#predictors = predictors of the next word in the sequence (its supervised, because we know the next word in the sequence)\n",
    "#labels = "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the ```create_model()``` function created above to initialize a model, telling the model the length of sequences and the total size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(max_sequence_len, total_words) #create_model == predefined function\n",
    "model.summary()\n",
    "\n",
    "#word embedding = 10 dimensional\n",
    "# lstm = 100\n",
    "#dropput = 100\n",
    "# dense  =\n",
    "\n",
    "\n",
    "#final prediction layer = 1137765"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training is exactly the same as last week, but instead of document labels, we're fitting the model to predict next word.\n",
    "\n",
    "*NB!* This will take some time to train! It took me 35 minutes on UCloud 32xCPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(predictors, #data (x)\n",
    "                    label, #y\n",
    "                    epochs=100, #arbitrarily, the more, the more accurate == timewise expensive \n",
    "                    batch_size=128, \n",
    "                    verbose=1) \n",
    "\n",
    "### NOTE!!! IF YOU RUN THIS AGAIN, IT HAS TRAINED ON 200 EPOCHS --> u must clear history or predefine model again (in the beginning of the script)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model has trained, we can then use this to generate *new text*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (generate_text(\"Russia\", 5, model, max_sequence_len)) #predefined function\n",
    "# the model has leaned form NY headlines, that what comes after \"Russia\" is probabilitywise closest to \"Wanted\" etc.\n",
    "# "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-trained word embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having the embedding layer as a trainable parameter, we can instead using a *pretrained word embedding* model like ```word2vec```.\n",
    "\n",
    "OTHER PPL HAVE TRAINED/CREATED BETTER WORD EMBEDDINGS, SO WE CAN ACTUALLT JUST IMPORT/TRANSFER/BORROW THESE AND USE THEM OUR SELVES.\n",
    "\n",
    "In the following examples, we're using [GloVe embeddings](https://nlp.stanford.edu/projects/glove/). These are trained a little differently from ```word2vec``` but they behave in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_glove_file = os.path.join(\"path/to/glove/vectors\")\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define some variables that we're going to use later.\n",
    "\n",
    "With hits and misses, we're counting how many words in the corpus vocabulary have a corresponding GloVe embedding; misses are the words which appear in our vocabulary but which do not have a GloVe embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = total_words\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer - notice that this is different\n",
    "    model.add(Embedding(\n",
    "            total_words,\n",
    "            embedding_dim,\n",
    "            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "            trainable=False,\n",
    "            input_length=input_len)\n",
    "    )\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(500))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, \n",
    "                    activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                    optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary() #the word embeddings are so much more large, e.g we have over a million trainable parameters!!\n",
    "#trainable param= \n",
    "#non-trainable param= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(predictors, \n",
    "                    label, \n",
    "                    epochs=100,\n",
    "                    batch_size=128, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (generate_text(\"china\", 30, model, max_sequence_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2 (default, Feb 28 2021, 17:03:44) \n[GCC 10.2.1 20210110]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
