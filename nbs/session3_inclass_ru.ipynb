{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to ```spaCy```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of different NLP frameworks that you're likely to encounter. The most popular and widely-used of these are:\n",
    "\n",
    "- ```NLTK``` (Natural Language Toolkit, old-school)\n",
    "- ```UDPipe``` (Neural network based, fast and light, but not super accurate)\n",
    "- ```CoreNLP``` and ```stanza``` (Created by the team at Stanford; academically robust)\n",
    "- ```spaCy``` production-ready, well-documented, state-of-the-art\n",
    "\n",
    "We'll be working with ```spaCy``` in this module, primarily because it's easy and intuitive, and also scales well.\n",
    "\n",
    "First thing we need to do is install ```spaCy``` and the language model that we want to use.\n",
    "\n",
    "From the command line, you should first make sure to run the setup script to install requirements:\n",
    "\n",
    "```shell \n",
    "bash setup.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing ```spaCy```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is import ```spaCy``` __and__ the language model that we want to use.\n",
    "\n",
    "Note that, if you want to use different langauges you want to use different language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spacy NLP class\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model now loaded, we can begin to do some very simple NLP tasks.\n",
    "\n",
    "Here, we create a spaCy object and assign it to the variable ```nlp```. This is the NLP pipeline that will do all our heavy lifting, using the trained model we've specified.\n",
    "\n",
    "Below, you can see what the pipeline does with a bit of sample text. Passing text to the nlp object gives us access to a bunch of properties, including tokens (words), parts of speech, named entities, and so on. Here's we two of them, tokens and entities. These objects, in turn, have certain methods attached to them. A full outline of available methods can be found in the spaCy docs.\n",
    "\n",
    "In this case, for all token objects, let's return the token itself (```token.text```); its part-of-speech tag (```token.pos_```); and the grammatical dependency relations between the tokens (```token.dep_```).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a single sentence example\n",
    "input_string = \"My name is Rikke and I have family in New York City.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat a new Doc object\n",
    "doc = nlp(input_string)\n",
    "#doc are complex object, that contains infromation from the text string and additional NLP stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking type of doc\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Rikke and I come from Denmark.\n"
     ]
    }
   ],
   "source": [
    "#Printng the Doc (we actually just get back a string, buuuut each of the tokens in the string, has additional grammar information )\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenize__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My\n",
      "name\n",
      "is\n",
      "Rikke\n",
      "and\n",
      "I\n",
      "have\n",
      "family\n",
      "in\n",
      "New\n",
      "York\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# tokenizing text\n",
    "for token in doc:\n",
    "    print(token.text) #using .text give every single token an attribute = POS, Name Entitiy etc.\n",
    "\n",
    "\n",
    "#Printing the tokens in the doc (string)\n",
    "#as we can see Spacy counts punctuation \n",
    "# and it seperated \"New York\"\n",
    "# when using spacy we work withing their framework and decisions of NLP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Trying some more attributes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 My PRON poss Number=Sing|Person=1|Poss=Yes|PronType=Prs\n",
      "1 name NOUN nsubj Number=Sing\n",
      "2 is AUX ROOT Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "3 Rikke PROPN attr Number=Sing\n",
      "4 and CCONJ cc ConjType=Cmp\n",
      "5 I PRON nsubj Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "6 have VERB conj Mood=Ind|Tense=Pres|VerbForm=Fin\n",
      "7 family NOUN dobj Number=Sing\n",
      "8 in ADP prep \n",
      "9 New PROPN compound Number=Sing\n",
      "10 York PROPN pobj Number=Sing\n",
      "11 . PUNCT punct PunctType=Peri\n"
     ]
    }
   ],
   "source": [
    "# find parts-of-speech and grammatical relations\n",
    "for token in doc:\n",
    "    print(token.i, token.text, token.pos_, token.dep_, token.morph) #prints:\n",
    "    #token index\n",
    "    # token text\n",
    "    # part of speech (POS) (note the _ is very important),\n",
    "    # depth of token (more gramatical information),\n",
    "    # morphology: tenses and number (singular&plural)\n",
    "        # if you dont have the _ you get a numerical value of the word class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NER__\n",
    "\n",
    "Extracting named entities from a ```spaCy``` doc requires an extra step, but nothing too challenging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rikke PERSON\n",
      "New York City GPE\n"
     ]
    }
   ],
   "source": [
    "# extracting NERs (NAMED ENTITIES)\n",
    "for ent in doc.ents: #ent = entity\n",
    "    print(ent.text, ent.label_) #prints:\n",
    "    #name entity\n",
    "    #label entity  (person & geopolitical entity) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Questions:__ \n",
    "\n",
    "1. What range of linguistic features is available beyond what we're looking at here? \n",
    "2. Are the same range of features available for all languages? Compare e.g. English and Danish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count distribution of linguistic features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create doc object__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a text file\n",
    "import os \n",
    "filename = os.path.join(\"..\", \"data\", \"example.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a doc object\n",
    "with open(filename, \"r\", encoding= \"utf-8\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first ORDINAL\n",
      "Twitter ORG\n",
      "Canaelan PERSON\n",
      "Taylor Swift PERSON\n",
      "Tottenham Hotspur PERSON\n",
      "Sheffield GPE\n",
      "Canaelan PERSON\n",
      "Advanced Impact Media Solutions ORG\n",
      "more than 30,000 CARDINAL\n",
      "Team Jorge WORK_OF_ART\n",
      "Israel GPE\n",
      "Tal Hanan PERSON\n",
      "Jorge PRODUCT\n",
      "UK GPE\n",
      "Information Commissioner’s Office ORG\n",
      "18 October 2020 DATE\n",
      "ICO ORG\n",
      "multimillion-pound MONEY\n",
      "PPE ORG\n",
      "Canaelan PERSON\n",
      "Twitter two days later DATE\n",
      "ICO ORG\n",
      "ICO ORG\n",
      "ICO ORG\n",
      "UK GPE\n",
      "one CARDINAL\n",
      "Team Jorge PERSON\n",
      "ICO ORG\n",
      "Hanan PERSON\n",
      "ICO ORG\n",
      "Team Jorge’s Aims ORG\n",
      "Hanan PERSON\n",
      "Twitter ORG\n",
      "LinkedIn ORG\n",
      "Facebook ORG\n",
      "Telegram ORG\n",
      "Gmail PERSON\n",
      "Instagram PERSON\n",
      "YouTube ORG\n",
      "Amazon ORG\n",
      "Airbnb ORG\n",
      "Hanan PERSON\n",
      "Team Jorge ORG\n",
      "Guardian ORG\n",
      "Le Monde ORG\n",
      "Der Spiegel PERSON\n",
      "2,000 CARDINAL\n",
      "Facebook ORG\n"
     ]
    }
   ],
   "source": [
    "#make doc from new text\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list\n",
    "entities = []\n",
    "\n",
    "# print all entitites in the doc (Text file)\n",
    "for ent in doc.ents:\n",
    "    entities.append(ent.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sheffield', '2,000', 'Twitter', 'multimillion-pound', 'YouTube', 'Twitter two days later', 'PPE', 'Advanced Impact Media Solutions', 'Gmail', 'Amazon', '18 October 2020', 'Team Jorge’s Aims', 'Israel', 'Canaelan', 'Tottenham Hotspur', 'one', 'LinkedIn', 'Telegram', 'Hanan', 'first', 'ICO', 'Tal Hanan', 'Taylor Swift', 'UK', 'more than 30,000', 'Facebook', 'Information Commissioner’s Office', 'Der Spiegel', 'Instagram', 'Guardian', 'Jorge', 'Le Monde', 'Airbnb', 'Team Jorge'}\n"
     ]
    }
   ],
   "source": [
    "#printing entities\n",
    "print(set(entities)) #set function is like \"unique\" in R (doing this we can e.g. find all unique persons in the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty list\n",
    "adjective_count = 0\n",
    "\n",
    "#count number of adjectives\n",
    "for token in doc:\n",
    "    if token.pos_ == \"ADJ\":\n",
    "        adjective_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "print(adjective_count) # note the relative frequency is more useful than the raw frequency =46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Relative frequency__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "727.8"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the relative frequency per 10,000 words\n",
    "relative_freq = (adjective_count/len(doc)) * 10000 #if out text was 10000 words long then we'd have 727 adjectives (per 10000 words = 727 adjectives)\n",
    "round(relative_freq, 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating neater outputs using ```pandas```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, all of our output from ```spaCy``` is in the form of lists. If we want to save these, it probably makes sense to have them saved in a more transferable format, such as CSV files or JSONs.\n",
    "\n",
    "One very easy way to do this with Python is by using the dataframe library ```pandas```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# a single sentence example\n",
    "input_string = \"My name is Rikke and I have family in New York City.\"\n",
    "\n",
    "# create spaCy doc\n",
    "doc = nlp(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = []\n",
    "\n",
    "for token in doc:\n",
    "    annotations.append((token.text, token.pos_)) #making them tuple so they can be appended\n",
    "    #append takes only 1 list, but we have 2, so we make them tuple just using brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>name</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rikke</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>have</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>family</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>New</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>York</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>City</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Token    POS\n",
       "0       My   PRON\n",
       "1     name   NOUN\n",
       "2       is    AUX\n",
       "3    Rikke  PROPN\n",
       "4      and  CCONJ\n",
       "5        I   PRON\n",
       "6     have   VERB\n",
       "7   family   NOUN\n",
       "8       in    ADP\n",
       "9      New  PROPN\n",
       "10    York  PROPN\n",
       "11    City  PROPN\n",
       "12       .  PUNCT"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spaCy doc to pandas dataframe\n",
    "data = pd.DataFrame(annotations, columns = [\"Token\", \"POS\"]) #making my \"annotations\" into DF <333\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "outpath = os.path.join(\"..\", \"data\", \"annotations.csv\")\n",
    "\n",
    "data.to_csv(outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "\n",
    "Spend some time exploring and familiarizing yourself with ```spaCy``` and ```pandas```. We'll come back to them quite a lot through this semester, so it will help to have a solid handle on how they function.\n",
    "\n",
    "When you are ready, head over to [Assignment 1](https://classroom.github.com/a/PdNi7nPv) which takes some of the skills you've learned last week and today. The task will be to count how many times certain linguistic features occur accross different documents, and to save those results in a clear and easy-to-understand way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
